{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'Date': 'Mon, 27 Apr 2020 00:57:33 GMT', 'Expires': '-1', 'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/html; charset=ISO-8859-1', 'P3P': 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"', 'Content-Encoding': 'gzip', 'Server': 'gws', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Set-Cookie': '1P_JAR=2020-04-27-00; expires=Wed, 27-May-2020 00:57:33 GMT; path=/; domain=.google.com; Secure, NID=203=YrdoWHxUEJ-YLWnExhH4EOas3sIlvrY8sHxS0AJNnnpgzYLpLFAlvuC_C7GBvGXg4YEYA3xpgvnj4IhkrQJ7AkeJS7vg59Kt8WkdHDoGX8vSrb70fHUifCWhzoUPt8EXA-ZXoiVT3VFtv855QHib6-sSqToYX1L8m4WqbRjDMHw; expires=Tue, 27-Oct-2020 00:57:33 GMT; path=/; domain=.google.com; HttpOnly', 'Alt-Svc': 'quic=\":443\"; ma=2592000; v=\"46,43\",h3-Q050=\":443\"; ma=2592000,h3-Q049=\":443\"; ma=2592000,h3-Q048=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,h3-T050=\":443\"; ma=2592000', 'Transfer-Encoding': 'chunked'}\n",
      "[<a class=\"gb1\" href=\"https://www.google.ca/imghp?hl=en&amp;tab=wi\">Images</a>, <a class=\"gb1\" href=\"https://maps.google.ca/maps?hl=en&amp;tab=wl\">Maps</a>, <a class=\"gb1\" href=\"https://play.google.com/?hl=en&amp;tab=w8\">Play</a>, <a class=\"gb1\" href=\"https://www.youtube.com/?gl=CA&amp;tab=w1\">YouTube</a>, <a class=\"gb1\" href=\"https://news.google.ca/nwshp?hl=en&amp;tab=wn\">News</a>, <a class=\"gb1\" href=\"https://mail.google.com/mail/?tab=wm\">Gmail</a>, <a class=\"gb1\" href=\"https://drive.google.com/?tab=wo\">Drive</a>, <a class=\"gb1\" href=\"https://www.google.ca/intl/en/about/products?tab=wh\" style=\"text-decoration:none\"><u>More</u> »</a>, <a class=\"gb4\" href=\"http://www.google.ca/history/optout?hl=en\">Web History</a>, <a class=\"gb4\" href=\"/preferences?hl=en\">Settings</a>, <a class=\"gb4\" href=\"https://accounts.google.com/ServiceLogin?hl=en&amp;passive=true&amp;continue=https://www.google.com/\" id=\"gb_70\" target=\"_top\">Sign in</a>, <a href=\"/search?ie=UTF-8&amp;q=thank+you+coronavirus+helpers&amp;oi=ddle&amp;ct=153582856&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjdtIGas4fpAhUSrZ4KHVqwCawQPQgD\"><img alt=\"To all the coronavirus helpers, thank you\" border=\"0\" height=\"154\" id=\"hplogo\" src=\"/logos/doodles/2020/thank-you-coronavirus-helpers-april-25-26-6753651837108777-law.gif\" title=\"To all the coronavirus helpers, thank you\" width=\"430\"/><br/></a>, <a href=\"/advanced_search?hl=en-CA&amp;authuser=0\">Advanced search</a>, <a class=\"NKcBbd\" href=\"https://www.google.com/url?q=https://www.youtube.com/watch%3Fv%3DQRRnoCHC8vg%26feature%3Dyoutu.be&amp;source=hpp&amp;id=19017511&amp;ct=3&amp;usg=AFQjCNH4SKhbUTVBUrDOf1c43CWv6VINmg&amp;sa=X&amp;ved=0ahUKEwjdtIGas4fpAhUSrZ4KHVqwCawQ8IcBCAU\" rel=\"nofollow\">Stay home to thank those who are helping us</a>, <a href=\"https://www.google.com/setprefs?sig=0_RWhdkqxOUFnCLqCRDBq3l83xolA%3D&amp;hl=fr&amp;source=homepage&amp;sa=X&amp;ved=0ahUKEwjdtIGas4fpAhUSrZ4KHVqwCawQ2ZgBCAc\">Français</a>, <a href=\"/intl/en/ads/\">Advertising Programs</a>, <a href=\"/services/\">Business Solutions</a>, <a href=\"/intl/en/about.html\">About Google</a>, <a href=\"https://www.google.com/setprefdomain?prefdom=CA&amp;prev=https://www.google.ca/&amp;sig=K_mvdn7WimXkTDhjpEoK3AZ1LYn_U%3D\">Google.ca</a>, <a href=\"/intl/en/policies/privacy/\">Privacy</a>, <a href=\"/intl/en/policies/terms/\">Terms</a>]\n",
      "\n",
      "\n",
      "<a href=\"/intl/en/about.html\">About Google</a>\n",
      "/intl/en/about.html\n"
     ]
    }
   ],
   "source": [
    "# YouTube Link:\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that you have both beautifulsoup and requests installed:\n",
    "\n",
    "#   pip install beautifulsoup4\n",
    "\n",
    "#   pip install requests\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Using the requests module, we use the \"get\" function\n",
    "\n",
    "# provided to access the webpage provided as an\n",
    "\n",
    "# argument to this function:\n",
    "\n",
    "result = requests.get(\"https://www.google.com/\")\n",
    "\n",
    "\n",
    "\n",
    "# To make sure that the website is accessible, we can\n",
    "\n",
    "# ensure that we obtain a 200 OK response to indicate\n",
    "\n",
    "# that the page is indeed present:\n",
    "\n",
    "print(result.status_code)\n",
    "\n",
    "\n",
    "\n",
    "# For other potential status codes you may encounter,\n",
    "\n",
    "# consult the following Wikipedia page:\n",
    "\n",
    "# https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "\n",
    "\n",
    "\n",
    "# We can also check the HTTP header of the website to\n",
    "\n",
    "# verify that we have indeed accessed the correct page:\n",
    "\n",
    "print(result.headers)\n",
    "\n",
    "\n",
    "\n",
    "# For more information on HTTP headers and the information\n",
    "\n",
    "# one can obtain from them, you may consult:\n",
    "\n",
    "# https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\n",
    "\n",
    "\n",
    "\n",
    "# Now, let us store the page content of the website accessed\n",
    "\n",
    "# from requests to a variable:\n",
    "\n",
    "src = result.content\n",
    "\n",
    "\n",
    "\n",
    "# Now that we have the page source stored, we will use the\n",
    "\n",
    "# BeautifulSoup module to parse and process the source.\n",
    "\n",
    "# To do so, we create a BeautifulSoup object based on the\n",
    "\n",
    "# source variable we created above:\n",
    "\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "# Now that the page source has been processed via Beautifulsoup\n",
    "\n",
    "# we can access specific information directly from it. For instance,\n",
    "\n",
    "# say we want to see a list of all of the links on the page:\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "print(links)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Perhaps we just want to extract the link that has contains the text\n",
    "\n",
    "# \"About\" on the page instead of every link. We can use the built-in\n",
    "\n",
    "# \"text\" function to access the text content between the <a> </a>\n",
    "\n",
    "# tags.\n",
    "\n",
    "for link in links:\n",
    "\n",
    "    if \"About\" in link.text:\n",
    "\n",
    "        print(link)\n",
    "\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b another-attribute=\"1\" id=\"verybold\">Test 2</b>\n",
      "Test 2\n",
      "<b another-attribute=\"1\" id=\"verybold\">This is another string</b>\n"
     ]
    }
   ],
   "source": [
    "# YouTube Link: https://www.youtube.com/watch?v=oDtLJEc5Ako\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "In this video, we will be going over BeautifulSoup objects, namely:\n",
    "\n",
    "    Tag, NavigableString, BeautifulSoup, and Comment\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To keep things simple and also reproducible, consider the following HTML code\n",
    "\n",
    "html_doc = \"\"\"\n",
    "\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "\n",
    "<body>\n",
    "\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; their names:\n",
    "\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\n",
    "\n",
    "\n",
    "<b class=\"boldest\">Extremely bold</b>\n",
    "\n",
    "<blockquote class=\"boldest\">Extremely bold</blockquote>\n",
    "\n",
    "<b id=\"1\">Test 1</b>\n",
    "\n",
    "<b another-attribute=\"1\" id=\"verybold\">Test 2</b>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('index.html', 'w') as f:\n",
    "\n",
    "    f.write(html_doc)\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "\n",
    "\n",
    "#print(soup.prettify())\n",
    "\n",
    "\n",
    "\n",
    "# Tag:\n",
    "\n",
    "\n",
    "\n",
    "# Finds the first occurrence of usage for a \"b\"\n",
    "\n",
    "# bold tag.\n",
    "\n",
    "#print(soup.b)\n",
    "\n",
    "\n",
    "\n",
    "# The \"find\" function also does the same, where it\n",
    "\n",
    "# only finds the first occurrence in the HTML doc\n",
    "\n",
    "# of a tag with \"b\".\n",
    "\n",
    "#print(soup.find('b'))\n",
    "\n",
    "\n",
    "\n",
    "# If we want to find all of the elements on the page\n",
    "\n",
    "# with the \"b\" tag, we can use the \"find_all\" function.\n",
    "\n",
    "#print(soup.find_all('b'))\n",
    "\n",
    "\n",
    "\n",
    "# Name:\n",
    "\n",
    "\n",
    "\n",
    "# This gives the name of the tag. In this case, the \n",
    "\n",
    "# tag name is \"b\".\n",
    "\n",
    "#print(soup.b.name)\n",
    "\n",
    "\n",
    "\n",
    "# We can alter the name and have that reflected in the\n",
    "\n",
    "# source. For instance:\n",
    "\n",
    "#tag = soup.b\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "#tag.name = \"blockquote\"\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# Attributes:\n",
    "\n",
    "\n",
    "\n",
    "#tag = soup.find_all('b')[2]\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# This specific tag has the attribute \"id\", which\n",
    "\n",
    "# can be accessed like so:\n",
    "\n",
    "#print(tag['id'])\n",
    "\n",
    "\n",
    "\n",
    "#tag = soup.find_all('b')[3]\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# We can even access multiple attributes that are\n",
    "\n",
    "# non-standard HTML attributes:\n",
    "\n",
    "#print(tag['id'])\n",
    "\n",
    "#print(tag['another-attribute'])\n",
    "\n",
    "\n",
    "\n",
    "# If we want to see all attributes, we can access them\n",
    "\n",
    "# as a dictionary object:\n",
    "\n",
    "#tag = soup.find_all('b')[3]\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "#print(tag.attrs)\n",
    "\n",
    "\n",
    "\n",
    "# These properties are mutable, and we can alter them\n",
    "\n",
    "# in the following manner.\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "#tag['another-attribute'] = 2\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# We can also use Python's del command for lists to\n",
    "\n",
    "# remove attributes:\n",
    "\n",
    "#del tag['id']\n",
    "\n",
    "#del tag['another-attribute']\n",
    "\n",
    "#print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# Multi-valued Attributes\n",
    "\n",
    "tag = soup.find_all('b')[3]\n",
    "\n",
    "print(tag)\n",
    "\n",
    "print(tag.string)\n",
    "\n",
    "\n",
    "\n",
    "# We can use the \"replace_with\" function to replace\n",
    "\n",
    "# the content of the string with something different:\n",
    "\n",
    "tag.string.replace_with(\"This is another string\")\n",
    "\n",
    "print(tag)\n",
    "\n",
    "\n",
    "\n",
    "# NavigableString\n",
    "\n",
    "\n",
    "\n",
    "# BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.whitehouse.gov/briefings-statements/joint-statement-president-donald-j-trump-president-vladimir-putin-russia-commemorating-75th-anniversary-meeting-elbe/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-signing-ceremony-h-r-266-paycheck-protection-program-health-care-enhancement-act/', 'https://www.whitehouse.gov/briefings-statements/text-letter-president-speaker-house-representatives-president-senate-71/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-approves-tennessee-disaster-declaration-7/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-approves-kentucky-disaster-declaration-5/', 'https://www.whitehouse.gov/briefings-statements/bill-announcement-95/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-remains-committed-providing-critical-relief-american-small-businesses-workers-healthcare-providers/', 'https://www.whitehouse.gov/briefings-statements/statement-president-armenian-remembrance-day/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-arbor-day-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-vice-president-pence-members-coronavirus-task-force-press-briefing-31/']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Let's obtain the links from the following website:\n",
    "\n",
    "# https://www.whitehouse.gov/briefings-statements/\n",
    "\n",
    "\n",
    "\n",
    "# One of the things this website consists of is records of presidential\n",
    "\n",
    "# briefings and statements.\n",
    "\n",
    "\n",
    "\n",
    "# Goal: Extract all of the links on the page that point to the \n",
    "\n",
    "# briefings and statements.\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "result = requests.get(\"https://www.whitehouse.gov/briefings-statements/\")\n",
    "\n",
    "src = result.content\n",
    "\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "urls = []\n",
    "\n",
    "for h2_tag in soup.find_all('h2'):\n",
    "\n",
    "    a_tag = h2_tag.find('a')\n",
    "\n",
    "    urls.append(a_tag.attrs['href'])\n",
    "\n",
    "\n",
    "\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.whitehouse.gov/briefings-statements/joint-statement-president-donald-j-trump-president-vladimir-putin-russia-commemorating-75th-anniversary-meeting-elbe/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-signing-ceremony-h-r-266-paycheck-protection-program-health-care-enhancement-act/', 'https://www.whitehouse.gov/briefings-statements/text-letter-president-speaker-house-representatives-president-senate-71/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-approves-tennessee-disaster-declaration-7/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-approves-kentucky-disaster-declaration-5/', 'https://www.whitehouse.gov/briefings-statements/bill-announcement-95/', 'https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-remains-committed-providing-critical-relief-american-small-businesses-workers-healthcare-providers/', 'https://www.whitehouse.gov/briefings-statements/statement-president-armenian-remembrance-day/', 'https://www.whitehouse.gov/briefings-statements/presidential-message-arbor-day-2020/', 'https://www.whitehouse.gov/briefings-statements/remarks-president-trump-vice-president-pence-members-coronavirus-task-force-press-briefing-31/']\n"
     ]
    }
   ],
   "source": [
    "# YouTube Link:\n",
    "\n",
    "\n",
    "\n",
    "# Let's obtain the links from the following website:\n",
    "\n",
    "# https://www.whitehouse.gov/briefings-statements/\n",
    "\n",
    "\n",
    "\n",
    "# One of the things this website consists of is records of presidential\n",
    "\n",
    "# briefings and statements.\n",
    "\n",
    "\n",
    "\n",
    "# Goal: Extract all of the links on the page that point to the \n",
    "\n",
    "# briefings and statements.\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "result = requests.get(\"https://www.whitehouse.gov/briefings-statements/\")\n",
    "\n",
    "src = result.content\n",
    "\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "\n",
    "\n",
    "urls = []\n",
    "\n",
    "for h2_tag in soup.find_all('h2'):\n",
    "\n",
    "    a_tag = h2_tag.find('a')\n",
    "\n",
    "    urls.append(a_tag.attrs['href'])\n",
    "\n",
    "\n",
    "\n",
    "print(urls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
